# From-Scratch Transformer (PyTorch Tensor Only)

Dá»± Ã¡n nÃ y hiá»‡n thá»±c má»™t mÃ´ hÃ¬nh **Transformer encoderâ€“decoder** hoÃ n chá»‰nh *tá»« sá»‘ 0* chá»‰ dÃ¹ng `torch.Tensor` thuáº§n:

- **KhÃ´ng dÃ¹ng** `torch.nn.Module`
- **KhÃ´ng dÃ¹ng** `torch.autograd`
- Tá»± viáº¿t:
  - Linear, ReLU, Softmax, LayerNorm, Multi-Head Attention, Add&Norm
  - Encoder / Decoder block, Encoder / Decoder stack, full Transformer
  - Optimizer **AdamW**
  - Training loop + overfitting test nhá» Ä‘á»ƒ verify gradient

Má»¥c tiÃªu: *hiá»ƒu rÃµ Transformer á»Ÿ má»©c toÃ¡n & code*, trÆ°á»›c khi dÃ¹ng `torch.nn` cho cÃ¡c project AI Engineer thá»±c chiáº¿n.

---

## ğŸ”§ TÃ­nh nÄƒng chÃ­nh

- **Core layers tá»± viáº¿t**:
  - `linear.py` â€“ Linear layer + backward tay
  - `activate_function/softmax.py` â€“ Softmax á»•n Ä‘á»‹nh sá»‘ + backward
  - `activate_function/relu.py` â€“ ReLU + backward
  - `self_attention.py` â€“ Self-Attention (self/cross) + backward
  - `multi_head_attention.py` â€“ Multi-Head Attention + backward
  - `mask_attention.py`, `mask_multi_head_attention.py`, `mask_utils.py` â€“ masked self-attention (causal / padding)
  - `add_norm.py` â€“ LayerNorm + Add&Norm (residual + layer norm)
  - `feed_forward_networks.py` â€“ FFN 2-layer
  - `embedding/embedding.py` â€“ simple embedding + positional encoding (sinusoidal)

- **Kiáº¿n trÃºc Transformer**:
  - `encoder.py` â€“ `BlockEncoder`
    - Multi-Head Self-Attention â†’ Add&Norm â†’ FFN â†’ Add&Norm
  - `bert.py` â€“ encoder stack (n_encoder Ã— BlockEncoder)
  - `decoder.py` â€“ `BlockDecoder`
    - Masked self-attention + Add&Norm  
    - Cross-attention vá»›i encoder (K, V) + Add&Norm  
    - FFN + Add&Norm
  - `decoders.py` â€“ decoder stack (n_decoder Ã— BlockDecoder)
  - `transformers.py` â€“ full Transformer encoderâ€“decoder:
    - encoder (Bert)
    - linear K/V cho encoder output
    - decoder stack
    - linear vocab + softmax output

- **Optimizer tá»± code**:
  - `optimizer/adamw.py` â€“ báº£n AdamW riÃªng:
    - moment `m`, `v`
    - bias correction `m_hat`, `v_hat`
    - decoupled weight decay: `param *= (1 - lr * weight_decay)`
  - Base class `Optimizer` vá»›i API giá»‘ng `torch.optim.Optimizer`:
    - `zero_grad()`
    - `step()`

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```text
.
â”œâ”€â”€ src
â”‚   â””â”€â”€ model
â”‚       â”œâ”€â”€ activate_function
â”‚       â”‚   â”œâ”€â”€ relu.py
â”‚       â”‚   â””â”€â”€ softmax.py
â”‚       â”œâ”€â”€ embedding
â”‚       â”‚   â””â”€â”€ embedding.py
â”‚       â”œâ”€â”€ optimizer
â”‚       â”‚   â””â”€â”€ adamw.py
â”‚       â”œâ”€â”€ add_norm.py
â”‚       â”œâ”€â”€ bert.py
â”‚       â”œâ”€â”€ decoder.py
â”‚       â”œâ”€â”€ decoders.py
â”‚       â”œâ”€â”€ encoder.py
â”‚       â”œâ”€â”€ feed_forward_networks.py
â”‚       â”œâ”€â”€ linear.py
â”‚       â”œâ”€â”€ mask_attention.py
â”‚       â”œâ”€â”€ mask_multi_head_attention.py
â”‚       â”œâ”€â”€ mask_utils.py
â”‚       â”œâ”€â”€ multi_head_attention.py
â”‚       â”œâ”€â”€ self_attention.py
â”‚       â””â”€â”€ transformers.py
â”œâ”€â”€ debug_components.py    # Script test tá»«ng component (attention, LN, FFN, ...)
â”œâ”€â”€ test_transformer.py    # Script test tá»•ng thá»ƒ Transformer
â”œâ”€â”€ train_overfit.py       # Script train overfit dataset toy
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

```

## ğŸ“Š Káº¿t quáº£ (Overfit toy dataset)

Dá»± Ã¡n Ä‘i kÃ¨m script `train_overfit.py` Ä‘á»ƒ kiá»ƒm tra xem toÃ n bá»™ mÃ´ hÃ¬nh (forward + backward + optimizer) cÃ³ hoáº¡t Ä‘á»™ng Ä‘Ãºng hay khÃ´ng báº±ng cÃ¡ch **overfit 4 sample nhá»**.

VÃ­ dá»¥ log cháº¡y thá»±c táº¿:

```text
Using device: cuda

==================================================
Báº¯t Ä‘áº§u training (overfit test)...
Dataset size: 4 samples
Sequence length: 5
==================================================

Epoch    0 | Loss: 2.293168 | Accuracy: 13.33%
Epoch   10 | Loss: 1.525540 | Accuracy: 46.67%
Epoch   20 | Loss: 0.873742 | Accuracy: 73.33%
Epoch   30 | Loss: 0.497384 | Accuracy: 80.00%
Epoch   40 | Loss: 0.254271 | Accuracy: 100.00%
Epoch   50 | Loss: 0.112127 | Accuracy: 100.00%
Epoch   60 | Loss: 0.075076 | Accuracy: 100.00%
Epoch   70 | Loss: 0.023652 | Accuracy: 100.00%
Epoch   80 | Loss: 0.015757 | Accuracy: 100.00%
Epoch   90 | Loss: 0.012851 | Accuracy: 100.00%
Epoch  100 | Loss: 0.011614 | Accuracy: 100.00%
...
Epoch  490 | Loss: 0.006171 | Accuracy: 100.00%

==================================================
Training hoÃ n thÃ nh!
==================================================

ğŸ“Š Káº¿t quáº£ cuá»‘i cÃ¹ng:
  - Loss Ä‘áº§u: 2.293168
  - Loss cuá»‘i: 0.006138
  - Giáº£m: 99.7%

ğŸ” Test predictions:
  Sample 1: Target=[3, 4, 5, 2], Pred=[3, 4, 5, 2] âœ…
  Sample 2: Target=[6, 7, 2], Pred=[6, 7, 2] âœ…
  Sample 3: Target=[3, 5, 7, 2], Pred=[3, 5, 7, 2] âœ…
  Sample 4: Target=[4, 6, 8, 2], Pred=[4, 6, 8, 2] âœ…

ğŸ‰ PASS: Model há»c Ä‘Æ°á»£c! Loss giáº£m xuá»‘ng < 0.1

```

## ğŸ“š TÃ i liá»‡u & bÃ i bÃ¡o liÃªn quan

Dá»± Ã¡n nÃ y dá»±a trÃªn cÃ¡c Ã½ tÆ°á»Ÿng kinh Ä‘iá»ƒn trong deep learning & Transformer:

- **Attention is All You Need**  
  *Ashish Vaswani, Noam Shazeer, Niki Parmar, et al., NeurIPS 2017*  
  BÃ i bÃ¡o giá»›i thiá»‡u kiáº¿n trÃºc Transformer, scaled dot-product attention, multi-head attention, positional encoding.  
  PDF / arXiv: https://arxiv.org/abs/1706.03762
  
- **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**  
  *Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018*  
  Sá»­ dá»¥ng encoder stack kiá»ƒu Transformer (giá»‘ng pháº§n `Bert` trong repo) cho cÃ¡c bÃ i toÃ¡n NLP.  
  ArXiv: https://arxiv.org/abs/1810.04805

- **Adam: A Method for Stochastic Optimization**  
  *Diederik P. Kingma, Jimmy Ba, 2014*  
  TrÃ¬nh bÃ y optimizer Adam â€“ ná»n táº£ng cho pháº§n cáº­p nháº­t moment `m`, `v` trong `AdamW`.  
  ArXiv: https://arxiv.org/abs/1412.6980

- **Decoupled Weight Decay Regularization (AdamW)**  
  *Ilya Loshchilov, Frank Hutter, ICLR 2019*  
  PhÃ¢n biá»‡t rÃµ Adam + L2 regularization vÃ  **AdamW** vá»›i weight decay tÃ¡ch rá»i â€“ chÃ­nh lÃ  kiá»ƒu update Ä‘Æ°á»£c hiá»‡n thá»±c trong `optimizer/adamw.py`.  
  ArXiv: https://arxiv.org/abs/1711.05101

